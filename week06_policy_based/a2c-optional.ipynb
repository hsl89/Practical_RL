{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb../xvfb: line 24: start-stop-daemon: command not found\r\n",
      ".\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    import os\n",
    "\n",
    "    os.system('apt-get install -y xvfb')\n",
    "    os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/xvfb -O ../xvfb')\n",
    "    os.system('apt-get install -y python-opengl ffmpeg')\n",
    "    os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "    os.system('python -m pip install -U pygame --user')\n",
    "\n",
    "    print('setup complete')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Advantage-Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
    "\n",
    "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames and stack them together) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
    "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function. Note that if you are using \n",
    "PyTorch and not using `tensorboardX` you will need to implement a wrapper that will log **raw** total rewards that the *unwrapped* environment returns and redefine the implemention of `nature_dqn_env` function here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of actions: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-7:\n",
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-6:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from atari_wrappers import nature_dqn_env, NumpySummaries\n",
    "\n",
    "nenvs = 8\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, \n",
    "                     summaries='Numpy')\n",
    "obs = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "print('num of actions: {}'.format(n_actions))\n",
    "assert obs.shape == (8, 84, 84, 4)\n",
    "assert obs.dtype == np.uint8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]]\n",
      "\n",
      "  [[ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]\n",
      "   [ 0  0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   ...\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]]\n",
      "\n",
      "  [[79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   ...\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]]\n",
      "\n",
      "  [[79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   ...\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]\n",
      "   [79 79 79 79]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# for evaluating the agent\n",
    "\n",
    "\n",
    "single_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=1, \n",
    "                     summaries='Numpy')\n",
    "\n",
    "print(single_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "one_env = gym.make('SpaceInvadersNoFrameskip-v4')\n",
    "one_obs = one_env.reset()\n",
    "print(one_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to implement a model that predicts logits and values. It is suggested that you use the same model as in [Nature DQN paper](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) with a modification that instead of having a single output layer, it will have two output layers taking as input the output of the last hidden layer. **Note** that this model is different from the model you used in homework where you implemented DQN. You can use your favorite deep learning framework here. We suggest that you use orthogonal initialization with parameter $\\sqrt{2}$ for kernels and initialize biases with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as torch\n",
    "# import torch as tf\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        \"\"\"\n",
    "        obs_dim: image dimension of the observation \n",
    "            The input image must have 2d dim \n",
    "            obs_dim x obs_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(obs_dim, kernel_size, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(out_size, kernel_size, stride)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(out_size, kernel_size, stride)\n",
    "\n",
    "        # size of the output tensor after convolution batch_size x 64 x out_size x out_size\n",
    "        self.linear = nn.Linear(64*out_size*out_size, 256)\n",
    "        \n",
    "        \n",
    "        # value head\n",
    "        self.value = nn.Linear(256, 1)\n",
    "        \n",
    "        # policy head\n",
    "        self.policy = nn.Linear(256, n_actions)\n",
    "        \n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        # qvalues = <YOUR CODE>\n",
    "        \n",
    "        #print('== type of input tensor ==', type(state_t))\n",
    "        #print(state_t.shape)\n",
    "        t = self.conv1(state_t)\n",
    "        t = F.relu(t)\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = t.view(state_t.shape[0], -1)\n",
    "        t = self.linear(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        values = self.value(t)\n",
    "        \n",
    "        # log proba of actions \n",
    "        logits = F.log_softmax(self.policy(t), dim=1)\n",
    "        return values, logits\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a dictionary of all the arrays that are needed to interact with an environment and train the model.\n",
    " Note that actions must be an `np.ndarray` while the other\n",
    "tensors need to have the type determined by your deep learning framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def parse_state(self, inputs):\n",
    "        #inputs = torch.Tensor(inputs, device=device)\n",
    "        #inputs = inputs.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # to float\n",
    "        inputs = inputs.astype(np.float32)\n",
    "        \n",
    "        # depth major\n",
    "        inputs = inputs.transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # normalize the inputs\n",
    "        mean = np.mean(inputs, axis=(2, 3), keepdims=True)\n",
    "        std = np.std(inputs, axis=(2, 3), keepdims=True)\n",
    "        inputs = (inputs- mean) / std\n",
    "        \n",
    "        # to torch tensor\n",
    "        inputs = torch.Tensor(inputs, device=device)\n",
    "        return inputs\n",
    "    \n",
    "    def act(self, inputs):\n",
    "        # input dim (8, 84, 84, 4)\n",
    "        # 8: number of parallel env \n",
    "        # 4: last 4 frames to make POMDP a MDP\n",
    "        # 84: frame dim\n",
    "        \n",
    "        # Should return a dict containing keys ['actions', 'logits', 'log_probs', 'values'].\n",
    "        # inputs are batched input from different environment\n",
    "        # each action in actions repr the action taken in one env\n",
    "        # Same thing with log probability\n",
    "        \n",
    "        # convert input to depth major      \n",
    "        inputs = self.parse_state(inputs)\n",
    "\n",
    "        # value estimates of states and \n",
    "        # log proba of actions at the states\n",
    "        values, logits = self.model(inputs)\n",
    "        # sample from log probabilities\n",
    "        m = Categorical(logits=logits)\n",
    "    \n",
    "        # sample an action for each env\n",
    "        actions = m.sample()\n",
    "        \n",
    "        return {\n",
    "            'actions': actions,\n",
    "            'log_probs': m.log_prob(actions),\n",
    "            'estimated_values': values\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def choose_action(self, inputs):\n",
    "        \"\"\"Choose action greedily in a testing env\"\"\"\n",
    "        inputs = self.parse_state(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, logits = self.model(inputs)\n",
    "            actions = torch.argmax(logits, dim=-1)\n",
    "        return actions\n",
    "        \n",
    "    def get_value(self, inputs):\n",
    "        \"\"\"estimate state value without keeping the computation graph\"\"\"\n",
    "        inputs = self.parse_state(inputs)\n",
    "        with torch.no_grad():\n",
    "            v, _ = self.model(inputs)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will pass the environment and policy to a runner that collects partial trajectories from the environment. \n",
    "The class that does is is already implemented for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys \n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the part of the model that predicts state values you will need to compute the value targets. \n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
    "Thus, we can implement and use `ComputeValueTargets` callable. \n",
    "The formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\left( \\sum_{t'=0}^{T - 1} \\gamma^{t'}r_{t+t'} \\right) + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "In implementation, however, do not forget to use \n",
    "`trajectory['resets']` flags to check if you need to add the value targets at the next step when \n",
    "computing value targets for the current step. You can access `trajectory['state']['latest_observation']`\n",
    "to get last observations in partial trajectory &mdash; $s_{t+T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeTargetValues:\n",
    "    \"\"\"Explicitly compute target value along the trajectory\n",
    "    in the forward view\n",
    "    \"\"\"\n",
    "    def __init__(self, policy, gamma=0.99):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __call__(self, trajectory):\n",
    "        # agent takes `nsteps` actions\n",
    "        nsteps = len(trajectory['actions'])\n",
    "        \n",
    "        # target values forward view\n",
    "        target_values = []\n",
    "        r = self.policy.get_value(\n",
    "            trajectory['state']['latest_observation']).squeeze(dim=1)\n",
    "        \n",
    "        for i in range(nsteps-1, -1, -1):\n",
    "            immediate_reward = trajectory['rewards'][i]\n",
    "            #print(immediate_reward)\n",
    "            immediate_reward = torch.Tensor(immediate_reward, \n",
    "                                            device=device)\n",
    "            r = immediate_reward + self.gamma * r * trajectory['resets'][i]\n",
    "            target_values.append(r)\n",
    "        \n",
    "        # reverse discounted rewards\n",
    "        trajectory['target_values'] = [\n",
    "            r for r in reversed(target_values)]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one trajectory\n",
    "import runners\n",
    "\n",
    "model = DQN(obs_dim=84, n_actions=6)\n",
    "policy = Policy(model)\n",
    "\n",
    "runner = runners.EnvRunner(\n",
    "    env, policy, nsteps=5,\n",
    "    transforms=[\n",
    "        ComputeTargetValues(policy),\n",
    "    ])\n",
    "\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing value targets we will transform lists of interactions into tensors\n",
    "with the first dimension `batch_size` which is equal to `T * nenvs`, i.e. you essentially need\n",
    "to flatten the first two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Modify trajectory inplace.\n",
    "        #<TODO: implement>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(trajectory):\n",
    "    # agent takes `nsteps` actions\n",
    "    nsteps = len(trajectory['actions'])\n",
    "\n",
    "    # discounted rewards forward view\n",
    "    discounted_rewards = []\n",
    "    r = policy.get_value(\n",
    "        trajectory['state']['latest_observation']).squeeze(dim=1)\n",
    "    \n",
    "    print(r.shape)\n",
    "    for i in range(nsteps-1, -1, -1):\n",
    "        immediate_reward = trajectory['rewards'][i]\n",
    "        print('== reward shape ==', immediate_reward.shape)\n",
    "        \n",
    "        print('== reset shape ==', trajectory['resets'][i].shape)\n",
    "        immediate_reward = torch.Tensor(immediate_reward, \n",
    "                                        device=device)\n",
    "        r = immediate_reward + 0.99 * r * trajectory['resets'][i]\n",
    "        discounted_rewards.append(r)\n",
    "        \n",
    "        print('== target reward shape ==', r.shape)\n",
    "    # reverse discounted rewards\n",
    "    trajectory['discounted_rewards'] = [\n",
    "        r for r in reversed(discounted_rewards)]\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into your lecture,\n",
    "[Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and [lecture](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) by Sergey Levine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy objective along one trajectory\n",
    "\n",
    "$$\n",
    "\\frac{1}{T}\\sum_{i=t_1}^{T} \\log(\\pi(a_i | s_i)) (R_i - V(s_i, \\theta_v))\n",
    "$$\n",
    "\n",
    "The length of the trajectory is $T$. \n",
    "\n",
    "Maximize likelihood of selected actions based on its advantage. \n",
    "\n",
    "For distributed RL, just take average among all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy loss along one trajectory\n",
    "\n",
    "# estimated value into one tensor of shape (n_step, n_workers)\n",
    "def transform_estimated_values(estimated_values):\n",
    "    \"\"\"\n",
    "    estimated_values: estimated values from env runner\n",
    "    \"\"\"\n",
    "    batched = [x.squeeze(1) for x in estimated_values]\n",
    "    return torch.vstack(batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476],\n",
       "        [0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476],\n",
       "        [0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476],\n",
       "        [0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455],\n",
       "        [0.0496, 0.0496, 0.0496, 0.0496, 0.0496, 0.0496, 0.0496, 0.0496]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_estimated_values(trajectory['estimated_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform target values r + \\gamma v(next state) into shape (n_steps, n_workers)\n",
    "trajectory['target_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_target_values(target_values):\n",
    "    \"\"\"transform target values r + \\gamma v(next state) into shape (n_steps, n_workers)\n",
    "    \n",
    "    target_values: target values along the trajectory for all workers from env runner\n",
    "    \"\"\"\n",
    "    return torch.vstack(target_values)\n",
    "\n",
    "transform_target_values(trajectory['target_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-1.7808, -1.7988, -1.7988, -1.8152, -1.8162, -1.8162, -1.8048, -1.7808],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-1.7988, -1.8162, -1.8162, -1.8048, -1.8048, -1.8048, -1.8048, -1.7371],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-1.8048, -1.8048, -1.8152, -1.8152, -1.7808, -1.8152, -1.7371, -1.8162],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-1.8050, -1.7814, -1.8169, -1.8169, -1.8050, -1.8162, -1.8050, -1.8050],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-1.7976, -1.7976, -1.7976, -1.7976, -1.7359, -1.7810, -1.7810, -1.8053],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory['log_probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7808, -1.7988, -1.7988, -1.8152, -1.8162, -1.8162, -1.8048, -1.7808],\n",
       "        [-1.7988, -1.8162, -1.8162, -1.8048, -1.8048, -1.8048, -1.8048, -1.7371],\n",
       "        [-1.8048, -1.8048, -1.8152, -1.8152, -1.7808, -1.8152, -1.7371, -1.8162],\n",
       "        [-1.8050, -1.7814, -1.8169, -1.8169, -1.8050, -1.8162, -1.8050, -1.8050],\n",
       "        [-1.7976, -1.7976, -1.7976, -1.7976, -1.7359, -1.7810, -1.7810, -1.8053]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_log_probs(log_probs):\n",
    "    \"\"\"\n",
    "    log_probs: log probabilities of actions for all workers from env runner\n",
    "    \"\"\"\n",
    "    return torch.vstack(log_probs)\n",
    "\n",
    "transform_log_probs(trajectory['log_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0856, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Compute policy loss\n",
    "target_values = transform_target_values(trajectory['target_values'])\n",
    "estimated_values = transform_estimated_values(trajectory['estimated_values'])\n",
    "\n",
    "# advantages\n",
    "adv = target_values - estimated_values\n",
    "\n",
    "# log probabilities\n",
    "log_probs = transform_log_probs(trajectory['log_probs'])\n",
    "\n",
    "# policy loss\n",
    "policy_loss = -torch.mean(log_probs * adv)\n",
    "\n",
    "print(policy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for critic\n",
    "\n",
    "Want to minimize the difference between estimated values and target values (just like q-learning)\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{i=t_1}^T (R_i - V(s_i, \\theta))^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0023, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute Value loss\n",
    "target_values = transform_target_values(trajectory['target_values'])\n",
    "estimated_values = transform_estimated_values(trajectory['estimated_values'])\n",
    "\n",
    "value_loss = torch.mean((target_values - estimated_values)**2)\n",
    "print(value_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package everything in an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self,\n",
    "                 policy,\n",
    "                 optimizer,\n",
    "                 value_loss_coef=0.25,\n",
    "                 entropy_coef=0.01,\n",
    "                 max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    def policy_loss(self, trajectory):\n",
    "        # You will need to compute advantages here.\n",
    "        \n",
    "        target_values = transform_target_values(trajectory['target_values'])\n",
    "        estimated_values = transform_estimated_values(trajectory['estimated_values'])\n",
    "\n",
    "        # advantages\n",
    "        adv = target_values - estimated_values\n",
    "\n",
    "        # log probabilities\n",
    "        log_probs = transform_log_probs(trajectory['log_probs'])\n",
    "\n",
    "        # policy loss\n",
    "        policy_loss = -torch.mean(log_probs * adv)\n",
    "        return policy_loss\n",
    "    \n",
    "    def value_loss(self, trajectory):\n",
    "        target_values = transform_target_values(trajectory['target_values'])\n",
    "        estimated_values = transform_estimated_values(trajectory['estimated_values'])\n",
    "        value_loss = torch.mean((target_values - estimated_values)**2)\n",
    "        return value_loss\n",
    "        \n",
    "    \n",
    "    def loss(self, trajectory):\n",
    "        return self.policy_loss(trajectory) + self.value_loss(trajectory)\n",
    "      \n",
    "    def step(self, trajectory):\n",
    "        loss = self.loss(trajectory)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled game rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent\n",
    "\n",
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an a game from start till done, returns per-game rewards \n",
    "    agent: an agent with .choose_action interface\n",
    "    test\n",
    "    \n",
    "    env: a testing env (no parallelism)\n",
    "    \"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        state = env.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward.item()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards\n",
    "\n",
    "\n",
    "\n",
    "model = DQN(obs_dim=84, n_actions=6)\n",
    "policy = Policy(model)\n",
    "\n",
    "single_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=1, \n",
    "                     summaries='Numpy')\n",
    "\n",
    "game_rewards = evaluate(policy, single_env, n_games=10)\n",
    "\n",
    "print('Sampled game rewards: {}'.format(game_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train your model. With reasonable hyperparameters training on a single GTX1080 for 10 million steps across all batched environments (which translates to about 5 hours of wall clock time)\n",
    "it should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last \n",
    "episodes in each environment in the batch) of about 600. You should plot this quantity with respect to \n",
    "`runner.step_var` &mdash; the number of interactions with all environments. It is highly \n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between \n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "* A2C loss\n",
    "\n",
    "For optimization we suggest you use RMSProp with learning rate starting from 7e-4 and linearly decayed to 0, smoothing constant (alpha in PyTorch and decay in TensorFlow) equal to 0.99 and epsilon equal to 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://stackoverflow.com/questions/58686400/can-not-get-pytorch-working-with-tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1000 is: -0.28313541412353516\n",
      "reward at iter 1000 is: 5.666666666666667\n",
      "loss at iter 2000 is: -0.4396350085735321\n",
      "reward at iter 2000 is: 5.666666666666667\n",
      "loss at iter 3000 is: -0.21510395407676697\n",
      "reward at iter 3000 is: 5.666666666666667\n",
      "loss at iter 4000 is: -0.7474871873855591\n",
      "reward at iter 4000 is: 5.666666666666667\n",
      "loss at iter 5000 is: -0.5137770175933838\n",
      "reward at iter 5000 is: 5.666666666666667\n",
      "loss at iter 6000 is: -0.7505146265029907\n",
      "reward at iter 6000 is: 6.0\n",
      "loss at iter 7000 is: -0.46364039182662964\n",
      "reward at iter 7000 is: 5.666666666666667\n",
      "loss at iter 8000 is: -0.21803009510040283\n",
      "reward at iter 8000 is: 5.666666666666667\n",
      "loss at iter 9000 is: -0.23464012145996094\n",
      "reward at iter 9000 is: 5.666666666666667\n",
      "loss at iter 10000 is: 0.12844032049179077\n",
      "reward at iter 10000 is: 5.666666666666667\n",
      "loss at iter 11000 is: -0.16093593835830688\n",
      "reward at iter 11000 is: 5.666666666666667\n",
      "loss at iter 12000 is: -0.10033877193927765\n",
      "reward at iter 12000 is: 5.666666666666667\n",
      "loss at iter 13000 is: -0.5104314088821411\n",
      "reward at iter 13000 is: 5.666666666666667\n",
      "loss at iter 14000 is: -0.7818995118141174\n",
      "reward at iter 14000 is: 0.0\n",
      "loss at iter 15000 is: -0.5841845273971558\n",
      "reward at iter 15000 is: 0.0\n",
      "loss at iter 16000 is: -0.4188072681427002\n",
      "reward at iter 16000 is: 0.0\n",
      "loss at iter 17000 is: -0.2507495880126953\n",
      "reward at iter 17000 is: 0.0\n",
      "loss at iter 18000 is: -0.3451836109161377\n",
      "reward at iter 18000 is: 0.0\n",
      "loss at iter 19000 is: -0.20809511840343475\n",
      "reward at iter 19000 is: 0.0\n",
      "loss at iter 20000 is: -0.39458492398262024\n",
      "reward at iter 20000 is: 0.0\n",
      "loss at iter 21000 is: -0.17306894063949585\n",
      "reward at iter 21000 is: 0.0\n",
      "loss at iter 22000 is: -0.34065425395965576\n",
      "reward at iter 22000 is: 0.0\n",
      "loss at iter 23000 is: -0.11972676217556\n",
      "reward at iter 23000 is: 0.0\n",
      "loss at iter 24000 is: -0.5578833818435669\n",
      "reward at iter 24000 is: 0.0\n",
      "loss at iter 25000 is: -0.5926379561424255\n",
      "reward at iter 25000 is: 0.0\n",
      "loss at iter 26000 is: -0.3520883619785309\n",
      "reward at iter 26000 is: 6.0\n",
      "loss at iter 27000 is: -0.7999760508537292\n",
      "reward at iter 27000 is: 0.0\n",
      "loss at iter 28000 is: -0.6213541030883789\n",
      "reward at iter 28000 is: 0.0\n",
      "loss at iter 29000 is: -0.2640545964241028\n",
      "reward at iter 29000 is: 0.0\n",
      "loss at iter 30000 is: -0.3060673773288727\n",
      "reward at iter 30000 is: 0.0\n",
      "loss at iter 31000 is: -0.42023608088493347\n",
      "reward at iter 31000 is: 3.3333333333333335\n",
      "loss at iter 32000 is: -0.1731145977973938\n",
      "reward at iter 32000 is: 0.0\n",
      "loss at iter 33000 is: -0.5664465427398682\n",
      "reward at iter 33000 is: 0.3333333333333333\n",
      "loss at iter 34000 is: -0.42541664838790894\n",
      "reward at iter 34000 is: 0.0\n",
      "loss at iter 35000 is: -0.36339154839515686\n",
      "reward at iter 35000 is: 5.666666666666667\n",
      "loss at iter 36000 is: -0.34774044156074524\n",
      "reward at iter 36000 is: 5.666666666666667\n",
      "loss at iter 37000 is: -0.001730799674987793\n",
      "reward at iter 37000 is: 5.666666666666667\n",
      "loss at iter 38000 is: -0.45391902327537537\n",
      "reward at iter 38000 is: 2.0\n",
      "loss at iter 39000 is: -0.2729361951351166\n",
      "reward at iter 39000 is: 0.6666666666666666\n",
      "loss at iter 40000 is: -0.22390542924404144\n",
      "reward at iter 40000 is: 0.0\n",
      "loss at iter 41000 is: -0.4164581000804901\n",
      "reward at iter 41000 is: 0.0\n",
      "loss at iter 42000 is: -0.42315906286239624\n",
      "reward at iter 42000 is: 5.666666666666667\n",
      "loss at iter 43000 is: -0.5107232928276062\n",
      "reward at iter 43000 is: 5.666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-20:\n",
      "Process Process-27:\n",
      "Process Process-22:\n",
      "Process Process-25:\n",
      "Process Process-24:\n",
      "Process Process-23:\n",
      "Process Process-21:\n",
      "Process Process-28:\n",
      "Process Process-26:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 132, in worker\n",
      "    worker_connection.send((ob, rew, done, info))\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/Projects/Practical_RL/week06_policy_based/env_batch.py\", line 127, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Users/hongshanli/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-af2a29dc7634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma2c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Practical_RL/week06_policy_based/runners.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latest_observation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Practical_RL/week06_policy_based/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_ended_episodes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Practical_RL/week06_policy_based/env_batch.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Practical_RL/week06_policy_based/env_batch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "model = DQN(obs_dim=84, n_actions=6)\n",
    "policy = Policy(model)\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, \n",
    "                     summaries='Numpy')\n",
    "\n",
    "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=1, \n",
    "                     summaries='Numpy')\n",
    "\n",
    "env.reset()\n",
    "test_env.reset()\n",
    "\n",
    "runner = runners.EnvRunner(\n",
    "    env, policy, nsteps=5,\n",
    "    transforms=[\n",
    "        ComputeTargetValues(policy),\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.RMSprop(policy.model.parameters(), \n",
    "                               lr=7e-4, \n",
    "                               alpha=0.99,\n",
    "                               eps=1e-5)\n",
    "\n",
    "a2c = A2C(policy, optimizer)\n",
    "writer = SummaryWriter(log_dir='a2c_log')\n",
    "n_iters = 1000*100\n",
    "\n",
    "losses = []\n",
    "rewards = []\n",
    "for i in range(1, n_iters+1):\n",
    "    trajectory = runner.get_next()\n",
    "    loss = a2c.step(trajectory)\n",
    "    if i % 1000 == 0:\n",
    "        print('loss at iter {} is: {}'.format(i, loss))\n",
    "        \n",
    "        # look at agent's performance on test env\n",
    "        game_rewards = evaluate(policy, test_env, n_games=3)\n",
    "        avg_reward = sum(game_rewards) / len(game_rewards)\n",
    "        print('reward at iter {} is: {}'.format(i, avg_reward))\n",
    "        \n",
    "        losses.append(loss)\n",
    "        rewards.append(avg_reward)\n",
    "        \n",
    "        writer.add_scalar('loss', loss, i)\n",
    "        writer.add_scalar('rewards', avg_reward, i)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda pytorch_p37",
   "language": "python",
   "name": "pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
