{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF0ZI_uSiXJC"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FybaAlqWiXJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb6b7d1-56bb-4fc1-c176-0e0663a70e56"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 146374 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzp7SNebiXJH"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYPNX1qiXJH"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK390Pb6iXJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "266abf39-324f-4391-ebf1-93212b9ba409"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5070328d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNafgD6vjwv0",
        "outputId": "e4d77386-a032-4c97-ac73-310d8e995317"
      },
      "source": [
        "print(state_dim[0], n_actions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfTvBpzhiXJH"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0U5vosRiXJI"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvfdZgxKiXJI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKIUWskriXJI"
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  #<YOUR CODE: define a neural network that predicts policy logits>\n",
        "  nn.Linear(state_dim[0], 200),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(200, 100),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(100, 2)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF5a0DtOiXJI"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vK7tBGZiXJI"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krvWnFy7iXJJ"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    # <YOUR CODE>\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        proba = F.softmax(model(states), dim=1)\n",
        "    return proba.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsWlCxfyiXJJ"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqdhScHEiXJJ"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iesJvdsViXJJ"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.argmax(action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POWsOvSZiXJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc1f73e-521d-4f62-ba0f-384cb2e9ed10"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)\n",
        "print(actions)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FymN8UeiXJK"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4UyaEgEiXJK"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    rev = reversed(rewards)\n",
        "    discounted_rewards = []\n",
        "    prev = 0.0\n",
        "    for r in rev:\n",
        "        cur = r + gamma * prev\n",
        "        discounted_rewards.append(cur)\n",
        "        prev = cur\n",
        "    return [d for d in reversed(discounted_rewards)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVzOvaZliXJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646ca158-a0c4-4dce-a8ee-d0ebab9f8fad"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0EgNio9pE32",
        "outputId": "fa707964-9ead-496a-8509-a6ef172c7197"
      },
      "source": [
        "get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziM3a1aiXJK"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yToIy_cTiXJL"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTrXrMFVvLLs"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJIUEvMniXJL"
      },
      "source": [
        "# Your code: define optimizers\n",
        "\n",
        "model = nn.Sequential(\n",
        "  #<YOUR CODE: define a neural network that predicts policy logits>\n",
        "  nn.Linear(state_dim[0], 128, bias=False),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(128, 512, bias=False),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(512, 2, bias=False),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / cumulative_returns.std()\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "    #print(probs)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    #print(log_probs_for_actions)\n",
        "    #print(actions)\n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    #entropy = <YOUR CODE>\n",
        "    entropy = log_probs_for_actions * cumulative_returns \n",
        "    loss = -torch.mean(entropy) * entropy_coef\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WVi0KPZiXJL"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuMZgIDxiXJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22a93ad-2cd0-4bb3-b089-3a55043f2b90"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=0.1) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:79.330\n",
            "mean reward:324.150\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Ege_shiXJM"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BW5GdiBiXJM"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csOkntYPiXJM",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.61.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "6665a7e0-110f-47de-9760-38bb1746d53f"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.61.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9BgffYciXJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9779b97-c51f-452b-b313-ca8c2ead2593"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'lihongshan8128@gmail.com', 'x7nP89CNkSSliuZV')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 351.32 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoaMtaHAiXJN"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}