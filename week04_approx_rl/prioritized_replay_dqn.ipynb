{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN With Prioritized Replay Buffer\n",
    "Use prioritized replay buffer to train a DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import atari_wrappers # adjust env\n",
    "from framebuffer import FrameBuffer # stack 4 consec images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "# create break-out env\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Crop the important part of the image, then resize to 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted env with 4 consec images stacked can be created\n"
     ]
    }
   ],
   "source": [
    "class PreprocessAtariObs(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.image_size = (1, 64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, self.image_size)\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "\n",
    "        # Here's what you need to do:\n",
    "        #  * crop image, remove irrelevant parts\n",
    "        #  * resize image to self.img_size\n",
    "        #     (use imresize from any library you want,\n",
    "        #      e.g. opencv, skimage, PIL, keras)\n",
    "        #  * cast image to grayscale\n",
    "        #  * convert image pixels to (0,1) range, float32 type\n",
    "\n",
    "        # crop the image \n",
    "        # remove the top part\n",
    "        img = img[50:]\n",
    "\n",
    "        # resize the image\n",
    "        img = cv2.resize(img, dsize=(self.image_size[1], self.image_size[2]))\n",
    "\n",
    "        # gray scale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # normalize to (0, 1)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # add channel dimension\n",
    "        return img[None]\n",
    "\n",
    "# adjust the env by some wrappers\n",
    "def PrimaryAtariWrap(env, clip_rewards=True):\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "\n",
    "    # This wrapper holds the same action for <skip> frames and outputs\n",
    "    # the maximal pixel value of 2 last frames (to handle blinking\n",
    "    # in some envs)\n",
    "    env = atari_wrappers.MaxAndSkipEnv(env, skip=4)\n",
    "\n",
    "    # This wrapper sends done=True when each life is lost\n",
    "    # (not all the 5 lives that are givern by the game rules).\n",
    "    # It should make easier for the agent to understand that losing is bad.\n",
    "    env = atari_wrappers.EpisodicLifeEnv(env)\n",
    "\n",
    "    # This wrapper laucnhes the ball when an episode starts.\n",
    "    # Without it the agent has to learn this action, too.\n",
    "    # Actually it can but learning would take longer.\n",
    "    env = atari_wrappers.FireResetEnv(env)\n",
    "\n",
    "    # This wrapper transforms rewards to {-1, 0, 1} according to their sign\n",
    "    if clip_rewards:\n",
    "        env = atari_wrappers.ClipRewardEnv(env)\n",
    "\n",
    "    # This wrapper is yours :)\n",
    "    env = PreprocessAtariObs(env)\n",
    "    return env\n",
    "    \n",
    "def make_env(clip_rewards=True, seed=None):\n",
    "    env = gym.make(ENV_NAME)  # create raw env\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    env = PrimaryAtariWrap(env, clip_rewards)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "print(\"adjusted env with 4 consec images stacked can be created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "class DuelingDQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        # nn.Flatten() can be useful\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(state_shape[1], kernel_size, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(out_size, kernel_size, stride)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(out_size, kernel_size, stride)\n",
    "\n",
    "        # size of the output tensor after convolution batch_size x 64 x out_size x out_size\n",
    "        self.linear = nn.Linear(64*out_size*out_size, 256)\n",
    "        \n",
    "        # advantage\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_actions)\n",
    "        )\n",
    "        \n",
    "        # state value\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        # qvalues = <YOUR CODE>\n",
    "        t = self.conv1(state_t)\n",
    "        t = F.relu(t)\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = t.view(state_t.shape[0], -1)\n",
    "        t = self.linear(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # compute advantage and state value as different heads\n",
    "        advantage = self.advantage(t)\n",
    "        value = self.value(t)\n",
    "        \n",
    "        qvalues = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "    \n",
    "# Evaluate the agent\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        reward = 0.0\n",
    "        s = env.reset()\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(\n",
    "                qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    device=device, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. '''\n",
    "    \n",
    "    objective of agent is \n",
    "    \\hat Q(s_t, a_t) = r_t + \\gamma Target(s_{t+1}, argmax_{a} Q(s_{t+1}, a))    \n",
    "    \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
    "\n",
    "    # for some torch reason should not make actions a tensor\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    \n",
    "    is_done = torch.tensor(\n",
    "        is_done,\n",
    "        device=device,\n",
    "        dtype=torch.float\n",
    "    )  # shape: [batch_size]\n",
    "    \n",
    "    is_not_done = 1 - is_done\n",
    "    \n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)\n",
    "   \n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "   \n",
    "    # best action in next state\n",
    "    next_best_actions = torch.argmax(agent(states), dim=1)\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "    \n",
    "        # compute the objective of the agent\n",
    "    next_state_values = predicted_next_qvalues[range(\n",
    "        len(actions)), next_best_actions]                          \n",
    "                                                     \n",
    "    # assert next_state_values.dim(\n",
    "    # == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    # target_qvalues_for_actions = <YOUR CODE>\n",
    "\n",
    "    target_qvalues_for_actions = rewards + next_state_values * is_not_done\n",
    "\n",
    "    # mean squared error loss adjusted by importance sampling weights to minimize\n",
    "    #loss = torch.mean(\n",
    "    #        weights * torch.pow(predicted_qvalues_for_actions - target_qvalues_for_actions.detach(), 2)\n",
    "    #)\n",
    "    \n",
    "    # return the TD-loss\n",
    "    \n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "    \n",
    "    return target_qvalues_for_actions - predicted_qvalues_for_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the memory need of the replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init DQN agent and play a total 10^4 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    sum_rewards = 0.0 \n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        action = agent.sample_actions(qvalues)[0] \n",
    "        next_s, r, done, _  = env.step(action)\n",
    "\n",
    "        exp_replay.add((s, action, r, next_s, done))\n",
    "        sum_rewards += r\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(100):\\n    state = env.reset()\\n    if not utils.is_enough_ram(min_available_gb=0.1):\\n        print(\"\"\"\\n            Less than 100 Mb RAM available. \\n            Make sure the buffer size in not too huge.\\n            Also check, maybe other processes consume RAM heavily.\\n            \"\"\"\\n             )\\n        break\\n    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\\n    if len(exp_replay) == 10**4:\\n        break\\nprint(len(exp_replay))\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import imp\n",
    "import replay_buffer\n",
    "imp.reload(replay_buffer)\n",
    "\n",
    "from replay_buffer import PrioritizedReplayBuffer\n",
    "\n",
    "\n",
    "#n_actions = env.action_space.n\n",
    "#state_shape = env.observation_space.shape\n",
    "\n",
    "agent = DuelingDQNAgent(state_shape=state_shape, n_actions=n_actions)\n",
    "exp_replay = PrioritizedReplayBuffer(10**4)\n",
    "\n",
    "'''\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "print(len(exp_replay))\n",
    "\n",
    "del exp_replay\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# env\n",
    "n_lives = 5\n",
    "\n",
    "\n",
    "# training params\n",
    "T = 1 # number of experiences to get from env before each update\n",
    "batch_size = 16\n",
    "total_steps = 3 * 10**1 # total steps to train the agent\n",
    "decay_steps = 10**1 # steps to decay the epsilon, \n",
    "                    # after the decay_steps, epsilon stops decaying\n",
    "                    # and the agent explores with a fixed probability\n",
    "max_grad_norm = 50 \n",
    "\n",
    "            \n",
    "refresh_target_network_freq = 5000 # freqency to update the target network\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "# agent \n",
    "gamma = 0.99 # discount factor\n",
    "init_epsilon = 1.0\n",
    "final_epsilon = 0.1\n",
    "\n",
    "# buffer\n",
    "buffer_size = 10**4\n",
    "\n",
    "# eval\n",
    "loss_freq = 50 \n",
    "eval_freq = 5000\n",
    "\n",
    "# logs \n",
    "ckpt_dir = 'logs'\n",
    "ckpt_file = 'prioritized_experience_replay_ckpt.pth'\n",
    "metrics_file = 'prioritized_experience_replay_metrics.pth'\n",
    "ckpt_freq = 10*5000 # Debug param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts training on cpu\n",
      "buffer size = 129, epsilon: 1.00000\n",
      "checkpointing ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-589eb0e0f5e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         check_shapes=True)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     '''\n",
      "\u001b[0;32m<ipython-input-48-aecb045e73bb>\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[0;34m(states, actions, rewards, next_states, is_done, agent, target_network, gamma, device, check_shapes)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m\\\u001b[0m\u001b[0mhat\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_t\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0mTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# shape: [batch_size, *state_shape]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# for some torch reason should not make actions a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "\n",
    "env = make_env(seed)\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DuelingDQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "target_network = DuelingDQNAgent(state_shape, n_actions).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "exp_replay = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "\n",
    "print(\"Starts training on {}\".format(next(agent.parameters()).device))\n",
    "\n",
    "# populate the buffer with 128 samples\n",
    "init_size = 128\n",
    "play_and_record(state, agent, env, exp_replay, init_size)\n",
    "\n",
    "for step in range(total_steps):\n",
    "    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "    \n",
    "    # play for $T time steps and cache the exprs to the buffer\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, T)\n",
    "    \n",
    "    b_idx, obses_t, actions, rewards, obses_tp1, dones, weights = exp_replay.sample(\n",
    "        batch_size)\n",
    "        \n",
    "    # td loss for each sample\n",
    "    td_loss = compute_td_loss(\n",
    "        states=obses_t, \n",
    "        actions=actions, \n",
    "        rewards=rewards, \n",
    "        next_states=obses_tp1, \n",
    "        is_done=dones,\n",
    "        agent=agent,\n",
    "        target_network=target_network,\n",
    "        gamma=gamma,\n",
    "        device=device,\n",
    "        check_shapes=True)\n",
    "                             \n",
    "    '''\n",
    "    A batch of samples from prioritized replay looks like:\n",
    "    (states, actions, rewards, next_states, weights, is_done)\n",
    "    weights here are importance sampling weights\n",
    "\n",
    "    Basically:\n",
    "        Loss = weights * MSE\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # compute MSE adjusted by importance sampling weights\n",
    "    # and backprop\n",
    "    weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    #print(weights, torch.pow(td_loss, 2))\n",
    "    loss = torch.mean(weights * torch.pow(td_loss, 2))\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # update the priorities of sampled exprs\n",
    "    exp_replay.batch_update(b_idx, np.abs(td_loss.detach().cpu().numpy()))\n",
    "    \n",
    "    # increase the importance sampling hyperparameter b gradually to 1\n",
    "    exp_replay.increment_b()\n",
    "    \n",
    "    if step % loss_freq == 0:\n",
    "        # save MSE without importance sampling\n",
    "        loss = torch.mean(torch.pow(td_loss, 2))\n",
    "        td_loss_history.append(loss.cpu().item())\n",
    "    \n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        target_network.load_state_dict(agent.state_dict())\n",
    "    \n",
    "    if step % eval_freq == 0:\n",
    "        mean_rw_history.append(evaluate(\n",
    "            make_env(clip_rewards=True, seed=step),\n",
    "            agent, n_games=3*n_lives, greedy=True\n",
    "        ))\n",
    "        \n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [make_env(seed=step).reset()]\n",
    "        )\n",
    "    \n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "        \n",
    "        print(\"buffer size = %i, epsilon: %.5f\" % \n",
    "             (len(exp_replay), agent.epsilon))\n",
    "        \n",
    "    \n",
    "    # TODO \n",
    "    # checkpointing\n",
    "    if step % ckpt_freq == 0:\n",
    "        print(\"checkpointing ...\")\n",
    "        \n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "            \n",
    "        # check point model and optimizer\n",
    "        checkpoint = {\n",
    "            \"step\": step,\n",
    "            \"agent\": agent.state_dict(),\n",
    "            \"epsilon\": agent.epsilon,\n",
    "            \"target_network\": target_network.state_dict(),\n",
    "            \"optimizer\": opt.state_dict(),\n",
    "            \"replay_buffer\": exp_replay\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(ckpt_dir, ckpt_file))\n",
    "    \n",
    "        # save the performance metric   \n",
    "        metrics = {\n",
    "            \"mean_rw_history\": mean_rw_history,\n",
    "            \"td_loss_history\": td_loss_history,\n",
    "            \"grad_norm_history\": grad_norm_history,\n",
    "            \"initial_state_v_history\": initial_state_v_history\n",
    "        }\n",
    "        \n",
    "        torch.save(metrics, os.path.join(ckpt_dir, metrics_file))\n",
    "        \n",
    "        \n",
    "# check point model and optimizer\n",
    "checkpoint = {\n",
    "    \"step\": step,\n",
    "    \"agent\": agent.state_dict(),\n",
    "    \"epsilon\": agent.epsilon,\n",
    "    \"target_network\": target_network.state_dict(),\n",
    "    \"optimizer\": opt.state_dict(),\n",
    "    \"replay_buffer\": exp_replay\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, os.path.join(ckpt_dir, ckpt_file))\n",
    "\n",
    "# save the performance metric   \n",
    "metrics = {\n",
    "    \"mean_rw_history\": mean_rw_history,\n",
    "    \"td_loss_history\": td_loss_history,\n",
    "    \"grad_norm_history\": grad_norm_history,\n",
    "    \"initial_state_v_history\": initial_state_v_history\n",
    "}\n",
    "\n",
    "torch.save(metrics, os.path.join(ckpt_dir, metrics_file))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda pytorch_p37",
   "language": "python",
   "name": "pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
