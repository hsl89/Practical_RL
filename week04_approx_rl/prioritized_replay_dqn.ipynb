{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN With Prioritized Replay Buffer\n",
    "Use prioritized replay buffer to train a DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import atari_wrappers # adjust env\n",
    "from framebuffer import FrameBuffer # stack 4 consec images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "# create break-out env\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Crop the important part of the image, then resize to 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted env with 4 consec images stacked can be created\n"
     ]
    }
   ],
   "source": [
    "class PreprocessAtariObs(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.image_size = (1, 64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, self.image_size)\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "\n",
    "        # Here's what you need to do:\n",
    "        #  * crop image, remove irrelevant parts\n",
    "        #  * resize image to self.img_size\n",
    "        #     (use imresize from any library you want,\n",
    "        #      e.g. opencv, skimage, PIL, keras)\n",
    "        #  * cast image to grayscale\n",
    "        #  * convert image pixels to (0,1) range, float32 type\n",
    "\n",
    "        # crop the image \n",
    "        # remove the top part\n",
    "        img = img[50:]\n",
    "\n",
    "        # resize the image\n",
    "        img = cv2.resize(img, dsize=(self.image_size[1], self.image_size[2]))\n",
    "\n",
    "        # gray scale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # normalize to (0, 1)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # add channel dimension\n",
    "        return img[None]\n",
    "\n",
    "# adjust the env by some wrappers\n",
    "def PrimaryAtariWrap(env, clip_rewards=True):\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "\n",
    "    # This wrapper holds the same action for <skip> frames and outputs\n",
    "    # the maximal pixel value of 2 last frames (to handle blinking\n",
    "    # in some envs)\n",
    "    env = atari_wrappers.MaxAndSkipEnv(env, skip=4)\n",
    "\n",
    "    # This wrapper sends done=True when each life is lost\n",
    "    # (not all the 5 lives that are givern by the game rules).\n",
    "    # It should make easier for the agent to understand that losing is bad.\n",
    "    env = atari_wrappers.EpisodicLifeEnv(env)\n",
    "\n",
    "    # This wrapper laucnhes the ball when an episode starts.\n",
    "    # Without it the agent has to learn this action, too.\n",
    "    # Actually it can but learning would take longer.\n",
    "    env = atari_wrappers.FireResetEnv(env)\n",
    "\n",
    "    # This wrapper transforms rewards to {-1, 0, 1} according to their sign\n",
    "    if clip_rewards:\n",
    "        env = atari_wrappers.ClipRewardEnv(env)\n",
    "\n",
    "    # This wrapper is yours :)\n",
    "    env = PreprocessAtariObs(env)\n",
    "    return env\n",
    "    \n",
    "def make_env(clip_rewards=True, seed=None):\n",
    "    env = gym.make(ENV_NAME)  # create raw env\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    env = PrimaryAtariWrap(env, clip_rewards)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "print(\"adjusted env with 4 consec images stacked can be created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "class DuelingDQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        # nn.Flatten() can be useful\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(state_shape[1], kernel_size, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(out_size, kernel_size, stride)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size, stride)\n",
    "        out_size = conv2d_size_out(out_size, kernel_size, stride)\n",
    "\n",
    "        # size of the output tensor after convolution batch_size x 64 x out_size x out_size\n",
    "        self.linear = nn.Linear(64*out_size*out_size, 256)\n",
    "        \n",
    "        # advantage\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_actions)\n",
    "        )\n",
    "        \n",
    "        # state value\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        # qvalues = <YOUR CODE>\n",
    "        t = self.conv1(state_t)\n",
    "        t = F.relu(t)\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = t.view(state_t.shape[0], -1)\n",
    "        t = self.linear(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # compute advantage and state value as different heads\n",
    "        advantage = self.advantage(t)\n",
    "        value = self.value(t)\n",
    "        \n",
    "        qvalues = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "    \n",
    "# Evaluate the agent\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        reward = 0.0\n",
    "        s = env.reset()\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(\n",
    "                qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. '''\n",
    "    \n",
    "    objective of agent is \n",
    "    \\hat Q(s_t, a_t) = r_t + \\gamma Target(s_{t+1}, argmax_{a} Q(s_{t+1}, a))\n",
    "    \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
    "\n",
    "    # for some torch reason should not make actions a tensor\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "    \n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)\n",
    "   \n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "   \n",
    "    # best action in next state\n",
    "    next_best_actions = torch.argmax(agent(states), dim=1)\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "    \n",
    "        # compute the objective of the agent\n",
    "    next_state_values = predicted_next_qvalues[range(\n",
    "        len(actions)), next_best_actions]                          \n",
    "                                                     \n",
    "    #assert next_state_values.dim(\n",
    "    #) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    # target_qvalues_for_actions = <YOUR CODE>\n",
    "\n",
    "    target_qvalues_for_actions = rewards + next_state_values * is_not_done\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda pytorch_p37",
   "language": "python",
   "name": "pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
